@ARTICLE{Mei2024-fc,
  title         = "{AIOS}: {LLM} Agent Operating System",
  author        = "Mei, Kai and Li, Zelong and Xu, Shuyuan and Ye, Ruosong and
                   Ge, Yingqiang and Zhang, Yongfeng",
  abstract      = "The integration and deployment of large language model
                   (LLM)-based intelligent agents have been fraught with
                   challenges that compromise their efficiency and efficacy.
                   Among these issues are sub-optimal scheduling and resource
                   allocation of agent requests over the LLM, the difficulties
                   in maintaining context during interactions between agent and
                   LLM, and the complexities inherent in integrating
                   heterogeneous agents with different capabilities and
                   specializations. The rapid increase of agent quantity and
                   complexity further exacerbates these issues, often leading
                   to bottlenecks and sub-optimal utilization of resources.
                   Inspired by these challenges, this paper presents AIOS, an
                   LLM agent operating system, which embeds large language
                   model into operating systems (OS) as the brain of the OS,
                   enabling an operating system ``with soul'' -- an important
                   step towards AGI. Specifically, AIOS is designed to optimize
                   resource allocation, facilitate context switch across
                   agents, enable concurrent execution of agents, provide tool
                   service for agents, and maintain access control for agents.
                   We present the architecture of such an operating system,
                   outline the core challenges it aims to resolve, and provide
                   the basic design and implementation of the AIOS. Our
                   experiments on concurrent execution of multiple agents
                   demonstrate the reliability and efficiency of our AIOS
                   modules. Through this, we aim to not only improve the
                   performance and efficiency of LLM agents but also to pioneer
                   for better development and deployment of the AIOS ecosystem
                   in the future. The project is open-source at
                   https://github.com/agiresearch/AIOS.",
  month         =  mar,
  year          =  2024,
  keywords      = "Serving LLM",
  archivePrefix = "arXiv",
  primaryClass  = "cs.OS",
  eprint        = "2403.16971"
}

@ARTICLE{Zhong2024-ad,
  title         = "{DistServe}: Disaggregating Prefill and Decoding for
                   Goodput-optimized Large Language Model Serving",
  author        = "Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu,
                   Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and
                   Zhang, Hao",
  abstract      = "DistServe improves the performance of large language models
                   (LLMs) serving by disaggregating the prefill and decoding
                   computation. Existing LLM serving systems colocate the two
                   phases and batch the computation of prefill and decoding
                   across all users and requests. We find that this strategy
                   not only leads to strong prefill-decoding interferences but
                   also couples the resource allocation and parallelism plans
                   for both phases. LLM applications often emphasize individual
                   latency for each phase: time to first token (TTFT) for the
                   prefill phase and time per output token (TPOT) of each
                   request for the decoding phase. In the presence of stringent
                   latency requirements, existing systems have to prioritize
                   one latency over the other, or over-provision compute
                   resources to meet both. DistServe assigns prefill and
                   decoding computation to different GPUs, hence eliminating
                   prefill-decoding interferences. Given the application's TTFT
                   and TPOT requirements, DistServe co-optimizes the resource
                   allocation and parallelism strategy tailored for each phase.
                   DistServe also places the two phases according to the
                   serving cluster's bandwidth to minimize the communication
                   caused by disaggregation. As a result, DistServe
                   significantly improves LLM serving performance in terms of
                   the maximum rate that can be served within both TTFT and
                   TPOT constraints on each GPU. Our evaluations show that on
                   various popular LLMs, applications, and latency
                   requirements, DistServe can serve 4.48x more requests or
                   10.2x tighter SLO, compared to state-of-the-art systems,
                   while staying within latency constraints for > 90\% of
                   requests.",
  month         =  jan,
  year          =  2024,
  keywords      = "Serving LLM",
  archivePrefix = "arXiv",
  primaryClass  = "cs.DC",
  eprint        = "2401.09670"
}

@ARTICLE{Fu2024-jj,
  title         = "{ServerlessLLM}: {Locality-Enhanced} Serverless Inference
                   for Large Language Models",
  author        = "Fu, Yao and Xue, Leyang and Huang, Yeqi and Brabete,
                   Andrei-Octavian and Ustiugov, Dmitrii and Patel, Yuvraj and
                   Mai, Luo",
  abstract      = "This paper presents ServerlessLLM, a locality-enhanced
                   serverless inference system for Large Language Models
                   (LLMs). ServerlessLLM exploits the substantial capacity and
                   bandwidth of storage and memory devices available on GPU
                   servers, thereby reducing costly remote checkpoint downloads
                   and achieving efficient checkpoint loading. ServerlessLLM
                   achieves this through three main contributions: (i) fast LLM
                   checkpoint loading via a novel loading-optimized checkpoint
                   format design, coupled with an efficient multi-tier
                   checkpoint loading system; (ii) locality-driven LLM
                   inference with live migration, which allows ServerlessLLM to
                   effectively achieve locality-driven server allocation while
                   preserving the low latency of ongoing LLM inference; and
                   (iii) locality-aware server allocation, enabling
                   ServerlessLLM to evaluate the status of each server in a
                   cluster and effectively schedule model startup time to
                   capitalize on local checkpoint placement. Our comprehensive
                   experiments, which include microbenchmarks and real-world
                   traces, show that ServerlessLLM surpasses state-of-the-art
                   systems by 10 - 200X in latency performance when running
                   various LLM inference workloads.",
  month         =  jan,
  year          =  2024,
  keywords      = "Serving LLM",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2401.14351"
}

@MISC{Qiu_undated-sq,
  title        = "[No title]",
  author       = "Qiu, Haoran",
  abstract     = "Ph.D. Candidate in Computer Science",
  howpublished = "\url{https://haoran-qiu.com/}",
  note         = "Accessed: 2024-5-16",
  keywords     = "Serving LLM"
}
